% -*- coding: utf-8 -*-

\section{Results}
\label{results}
\subsection{Introduction}
In this section I will discuss the results of the project. I will start by discussing the speed of the implemented features, and then talk about volume.

\subsection{Implemented features}
I have successfully implemented the RSS BV as well as its dependent class, a rectangle in 3D. 
All of the algorithms described in the last section has been implemented.
Furthermore I have implemented all the required methods for the 2 classes.

\subsection{Test data}
For the test results I have used 2 data sets. The randomly generated data, and a sample of real world proteins, supplied to me by Rasmus Fonesca.

The random test data will be used as a general stress test of the implemented algorithm (since I will be able to generate a large number of point sets), while the real world data will be used to gauge its usefulness in practice.

\subsubsection{Randomly generated test data}
For the randomly generated test data, has been made by constructing 2 arrays of 1000 point sets with 25 points in each set. The points have all been randomly created by using Java's built in Random function and Point3d's own Random constructor.

I maintain 2 arrays for each type of BV I create, with the intention of checking each element from the first array against all elements in the second array. This is done in order to ensure that in practice a BV is never checked against itself, and also so that I am quickly able to create large number of tests, without having to use too much system memory. \\ 

For each point set in each array, I have created a 3 new BV's, one for
each of the types OBB, LSS, and RSS. Each BV contains the all points
in the point set it was created from. Thus I create 2000 (split into 2 arrays of 1000 each)
new BV of each type. \\ 

\subsubsection{Real world data}
The real world data has been generated by parsing a text file containing definitions for real world proteins. Each Protein contains 10 chunk-chains, each chunk-chain is defined by a set of points. For each chunk-chain point set I will create a BV, which I will test for overlap against all other chunk-chains in the same protein (since this sub test is to check its ability to handle real world data, it would make little sense to test it against the chunk-chains from other proteins). 
I will perform this test for OBBs, LSS' and RSS'

\subsection{Data extracted from tests}

I will compare the efficiency of the tests based on the metrics of BV volumes, and the wall clock time required to check whether 2 BV's overlaps or not. I feel having both volume and wall clock time is necessary, since it might give hints whether certain BV properties can mean measurable time savings, despite being a worse fit. 

I perform 2 test: 
\begin{description}
\item[Overlap:] I test the wall clock time required to perform the overlap test,
  for all possible BV combinations, for all 3 types of BV. The creation of the BV's are as described above.
 
\item[Volume:] I find the average volume of the individual BV's that cover a single point set. For the randomly generated data I will also generate the average of all possible BV combinations that can be created from combining 2 BV's. I have not done this for the real world data, as the concept of average for combined BV's makes no sense, since BV's will only be combined within their own protein.
\end{description}

All tests have been executed on a computer running Windows XP Pro (Service pack 3) with a 8x3.06Ghz Core i7-950 and has 6 GB of RAM.s

\subsection{Overlap detection}

\subsubsection{Randomly generated data}

\begin{table}
\input{tableData/allTime.tex}
\caption{\label{overlap-table}The table of the time used for the
  different overlaps checks. All of the times are in wall clock time seconds. The
  check reading ``RSS no-min'' is a RSS overlap check that only runs the axis separation test, and no minimum distance check}
\end{table}

As it is clear from table \ref{overlap-table}, the current RSS overlap algorithm is slower than both the one for OBB's, and LSS'. However, seeing that the RSS both has a much larger average volume than the other 2, could indicate that the properties of the RSS either makes up for its greater volume. It is very interesting that the RSS no-min uses slightly less time than OBB test, despite the fact that they both use the axis-separation method.

It is worth noting that for the RSS, the majority of the used time comes from the Axis-separation check (since it is clear that RSS no-min takes over half the time a regular RSS test takes).

\subsubsection{Volume}
\begin{table}
\input{tableData/volumes.tex}
\caption{\label{volume-table} The average volume needed by the
  different BV to contain the points. The first row of values are the average
  volumes for each of the 2000 different BV that are produced, while
  the second row of values are the average volumes of the 1.000.000
  different combinations of BV's.}
\end{table}

From table \ref{volume-table} it is clear that the current implementation of the RSS is worse than both OBBs and LSS', both combined and on average.

\subsection{Real world data}
\label{realWorldData}
\begin{table}
\input{tableData/realData.tex}
\caption{\label{overlap-realtable}The table of the time used for the
  different overlaps checks. All of the times are in wall clock time seconds. The
  check reading ``RSS no-min'' is a RSS overlap check that only runs the axis separation test, and no minimum distance check}
\end{table}

It is clear from table \ref{overlap-realtable}, that the current implementation of RSS overlap algorithm is much slower than the ones for  OBBs, and  LSS'. Furthermore it is clear that in this case the main slowdown comes from the minimum distance check, and not the Axis-separation, since the No-min tests for the RSS perform just as fast as the one for the OBB, despite having a greater volume. It is furthermore interesting to note that the overlap check that has the minimum distance check reports slightly more overlaps, than the test which does not. Since the RSS no-min tests uses the Axis-separation method which always detects an overlap, this indicates that the current implementation of the minimum distance algorithm reports overlaps which does not exits. However, the extra number of possible overlaps detected might not prove a too significant a performance hit in practice.

It is worth noting, that unlike the last check, the main time expenditure for the program lies with the minimum distance check (Since the RSS no-min test clearly takes under half the time of the normal RSS overlap test).

\subsubsection{Volume}
\begin{table}
\input{tableData/realVolume.tex}
\caption{\label{volume-realtable} The average volume needed by the
  different BV to contain the points.}
\end{table}

Interestingly table \ref{volume-realtable} gives a far worse average for the RSS BV, compared to the other 2 BV's than from the randomly generated data. This would indicate that it would be worth looking into reducing the volume.

\subsection{Conclusion}
I have in this section shown that the current implementation of the RSS performs worse than both the OBB and the LSS on both metrics of volume and wall clock time when performing overlap detection.

The differences in proportions of time spent on Minimum distance and Axis-separation for the RSS', for the 2 types of test data, can most likely be attributed to the fact that with the randomly generated data, every single BV overlaps (see table \ref{volume-table}), while with the real world data (see table \ref{volume-realtable}), far fewer does. As I explained in section \ref{sepAxis}, the Axis-separation algorithm terminates as soon it finds a axis that separates the 2 BV's, but in the case where none exists, it will do all 15 tests. Since we have established that all BV's for the randomly generated data overlaps, it is clear that all 15 checks for the RSS must be performed.  

If the RSS BV is to be used in practice, these limitations must be overcome. There is furthermore a clear indication that for real world data, the bottleneck of the current implementation lies in the Minimum distance algorithm, which should be optimized.
